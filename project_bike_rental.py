# -*- coding: utf-8 -*-
"""Project_Bike_rental.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14o7FKE0jAdOyy6hiW4UTE4zvaOQogPhX

# **Bike-Sharing Rental**

#**Problem Statement :**
The business problem is to ensure a stable supply of rental bikes in urban cities by predicting the demand for bikes.By providing a stable supply of rental bikes, the system can enhance mobility comfort for the public and reduce waiting time,leading to greater customer satisfaction and accurately predicting bike demand can help bike sharing companies optimize operations including bike availability,pricing,strategies,and marketing efforts by considering demand Based on various external factors such as weather,season,holiday etc..,

## **1.Exploratory Data Analysis (EDA):**
•Describe the dataset: Understand the structure, columns, and data types.  
•Clean the data: Check for invalid records, missing values, duplicated records, and outliers.  
•Handle missing values: Detect missing values and apply appropriate imputation techniques.  
•Detect outliers: Identify outliers and decide whether to remove or transform them.  

## **2.Data Visualization:**
•Creating visualizations such as scatter plots, line plots, and bar plots to explore relationships between variables.  
•Plotting time series data to analyze trends and seasonality.  
•Using heatmaps or correlation matrices to understand the correlation between different features.  
•Utilizing seaborn, matplotlib, or other libraries for generating informative and visually appealing plots.  

##**3.Feature Engineering:**
•Generating new features from existing ones to improve model performance.  
•Handling categorical variables through techniques like one-hot encoding or label encoding.  
•Scaling or normalizing numerical features to ensure uniformity in their ranges.  
•Incorporating domain knowledge to create meaningful features that capture important aspects of the data.

##**4.Model Building:**
•	Selecting appropriate machine learning algorithms such as Decision Tree, Random Forest, and Gradient Boosting Regression for predicting bike rental demand.  
•Splitting the dataset into training and testing sets for model evaluation.  
•Training various models using the training data and evaluating their performance using metrics like RMSE, MAE, or R-squared.  
•Fine-tuning model parameters to optimize performance.

##**5.Hyperparameter Tuning:**
•Conducting hyperparameter tuning for Decision Tree,Random Forest,and Gradient Boosting Regression using techniques like grid search or random search to find the best combination of model parameters.  
•Evaluating model performance with different hyperparameter values using cross-validation techniques.  
•Balancing model complexity and performance to prevent overfitting or underfitting.

##**6.Model Evaluation:**
•Evaluation metrics for regression models: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (R^2), etc.  
•Compare the performance of different models to select the best one.

##**7.Model Deployment:**
•Once the best model is selected, deploy it in a production environment where it can be used to make predictions.  
•Monitor the performance of the deployed model over time and update it as needed.

##Dataset Dictionary:
1.Instant: Index number  
2.Dteday: Date(Format: YYYY-MM-DD)  
3.Season: Season Name  
4.Yr: Year  
5.Month: Month(1-12)(Jan-Dec)  
6.Hr: Hour(0 to 23)  
7.Holiday: Whether the holiday is there or not  
8.Weekday: Day of the week  
9.Workingday: Whether it is a working day or not  
10.Weathersit: Weather situation  
11.Temp: Normalized temperature in Celsius  
12.Atemp: Normalized feeling temperature  
13.Hum: Normalized humidity. The Values are divided by 100  
14.Windspeed: Normalized Wind speed. Values are divided by 67  
15.Casual: Count of casual users  
16.Registered: Number of registered users  
17.Cnt: Count of total rental biked including both casual and registered
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns

bike_df=pd.read_csv("C://Users//SAI NITHIN//OneDrive//Desktop//Project Documents//bike_rent.csv",index_col='instant')

bike_df

"""# **1.	Exploratory Data Analysis (EDA):**

###Describe the dataset: Understand the structure, columns, and data types.
"""

bike_df.head()

bike_df.tail()

#Structure
bike_df.shape

#Check for unique values
bike_df.nunique()

bike_df.info()

#Datatypes
bike_df.dtypes

categorical_columns=bike_df.select_dtypes(include='object').columns
categorical_columns

numerical_columns=bike_df.select_dtypes(include=np.number).columns
numerical_columns

bike_df.describe()

"""###Clean the data: Check for invalid records, missing values, duplicated records, and outliers."""

#Check for invalid records
invalid_records=bike_df.isin(['?','-','N/A','None','']).sum()
invalid_records

#Replace invalid records with NAN
bike_df.replace(['?','-','N/A','None',''],np.nan,inplace=True)

#Check for missing values
bike_df.isnull().sum()

#Check for Duplicated Records
bike_df.duplicated().sum()

#No Duplicates Found

#Check for Outliers
numerical_columns = ['yr','mnth','hr','weekday','temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']

#Convert specified columns to numeric, handling errors by coercing them to NaN
for column in ['temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']:
    bike_df[column] = pd.to_numeric(bike_df[column], errors='coerce')

# Redefine numerical_columns to ensure it only contains numeric columns
numerical_columns = bike_df.select_dtypes(include=np.number).columns

def detect_outliers_iqr(data, column):
    Q1 = bike_df[column].quantile(0.25)
    Q3 = bike_df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers_indices = ((bike_df[column] < (lower_bound)) | (bike_df[column] > (upper_bound))).sum()
    return outliers_indices

for col in numerical_columns:
    outliers_indices=detect_outliers_iqr(bike_df,col)
    print(f"Number of outliers in {col}: {outliers_indices}")

"""###Handle missing values: Detect missing values and apply appropriate imputation techniques."""

#Check for any missing values(Before)
bike_df.isnull().sum()

#Handle missing values for both Numerical(mean) and Categorical columns(mode)
for col in bike_df.columns:
    if bike_df[col].isnull().sum() > 0:  # Check if the column has missing values
        if bike_df[col].dtype in ['float64', 'int64']:  #For Numeric column
            bike_df[col].fillna(bike_df[col].mean(), inplace=True)
        else:  #For Categorical column
            bike_df[col].fillna(bike_df[col].mode()[0], inplace=True)

#Check for any missing values(After)
bike_df.isnull().sum()

"""###Detect outliers: Identify outliers and decide whether to remove or transform them."""

# Select numerical columns
numerical_columns = ['temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']

for col in numerical_columns:
    Q1 = bike_df[col].quantile(0.25)
    Q3 = bike_df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

#Identify Outliers
outliers = bike_df[(bike_df[col] < lower_bound) | (bike_df[col] > upper_bound)]
outliers

#First method:Remove Outliers
bike_df=bike_df[(bike_df[col] >= lower_bound) & (bike_df[col] <= upper_bound)]

#Second method:Cap Outliers(transform)
bike_df[col]=np.clip(bike_df[col],lower_bound,upper_bound)

"""# **2.Data Visualization:**

###Creating visualizations such as scatter plots, line plots, and bar plots to explore relationships between variables.
"""

import matplotlib.pyplot as plt
import seaborn as sns

#SCATTER PLOT
#Temperature vs Bike Rentals:
plt.figure(figsize=(10,8))
sns.scatterplot(x='temp', y='cnt', data=bike_df)
plt.title('Temperature vs. Bike Rentals')
plt.xlabel('Temperature')
plt.ylabel('Bike Rentals')
plt.show()

#Key Insight in Temperature vs Bike Rentals
#Rentals increase with moderate temperatures,peaking at an ideal range(e.g., 20–30°C) and Extremely high or low temperatures reduce rentals due to user discomfort.

#Humidity vs Bike Rentals:
plt.figure(figsize=(10,8))
sns.scatterplot(x='hum', y='cnt', data=bike_df)
plt.title('Humidity vs. Bike Rentals')
plt.xlabel('Humidity')
plt.ylabel('Bike Rentals')
plt.show()

#Key Insight in Humidity vs Bike Rentals
#Higher humidity (above 80%) might negatively affect bike rentals.Mostly Moderate humidity levels (40–70%) may correlate with higher rentals as conditions are more comfortable.

#Windspeed vs Bike Rentals:
plt.figure(figsize=(10,8))
sns.scatterplot(x='windspeed', y='cnt', data=bike_df)
plt.title('Windspeed vs. Bike Rentals')
plt.xlabel('Windspeed')
plt.ylabel('Bike Rentals')
plt.show()

#Key Insights in Windspeed vs Bike Rentals
#Rentals decline as windspeed increases,especially above a certain threshold(e.g.,>20 km/h),due to challenging riding conditions and Lower wind speeds are more favorable for bike users.

#Hour vs Rentals
plt.figure(figsize=(10,8))
sns.barplot(x='hr', y='cnt', data=bike_df)
plt.title('Hour vs. Rentals')
plt.xlabel('Hour')
plt.ylabel('Rentals')
plt.show()

#Key Insights in
#Rentals spike during commute hours (8–10 AM,5–7 PM) on working days.In Non-working days it peaks in the afternoon (12 PM–3 PM), likely due to leisure activities.

#LINE PLOT
#Bike Rentals Over Time:
plt.figure(figsize=(10,8))
sns.lineplot(x='dteday',y='cnt',data=bike_df)
plt.title('Daily Bike Rentals Trend')
plt.xlabel('Date')
plt.ylabel('Total Bike Rentals')
plt.xticks(rotation=45)
plt.show()

#Key Insights
#Rentals may show an increasing trend over time due to the growing popularity of the service.
#Clear seasonality: higher rentals in spring/summer and lower in winter.

#Hourly Rentals by Working Day:
plt.figure(figsize=(10,8))
sns.lineplot(x='hr', y='cnt',hue='workingday',data=bike_df)
plt.title('Hourly Rentals by Working Day')
plt.xlabel('Hour')
plt.ylabel('Total Bike Rentals')
plt.show()

#Key Insights
#On Working Days: Rentals spike during commute hours (8–10 AM, 5–7 PM).
#On Non-Working Days: Peak usage during leisure hours (12–3 PM).

#Hourly Rentals by Season
plt.figure(figsize=(10,8))
sns.lineplot(x='hr', y='cnt',hue='season',data=bike_df)
plt.title('Hourly Rentals by Season')
plt.xlabel('Hour')
plt.ylabel('Total Bike Rentals')
plt.show()

#Key Insight:
#In Spring/Summer: Higher overall rentals with mid-day peaks.
#In Winter/Fall: Lower rentals throughout the day, indicating seasonal weather impacts.

#BAR PLOT
#Average Rentals by Season:
plt.figure(figsize=(10,8))
sns.barplot(x='season', y='cnt', data=bike_df)
plt.title('Average Rentals by Season')
plt.xlabel('Season')
plt.ylabel('Average Rentals')
plt.show()

#KEY INSIGHTS
#Spring and summer have higher bike rentals due to favorable weather where as winter shows the lowest rentals due to harsh weather conditions.

#Rentals by Weather Condition
plt.figure(figsize=(10,8))
sns.barplot(x='weathersit', y='cnt', data=bike_df)
plt.title('Rentals by Weather Condition')
plt.xlabel('Weather Condition')
plt.ylabel('Total Rentals')
plt.show()

#Key Insight:
#Clear weather leads to significantly higher rentals and Poor weather(e.g.,rain,snow) results in fewer rentals.

#Bike Rentals on Holidays:
plt.figure(figsize=(10,8))
sns.barplot(x='holiday', y='cnt', data=bike_df)
plt.title('Bike Rentals on Holidays')
plt.xlabel('Holiday')
plt.ylabel('Total Rentals')
plt.show()

#Key Insight:
#Rentals are slightly lower on holidays, as fewer people commute and Casual rentals may increase depending on the holiday type.

#Bike Rentals by Month
plt.figure(figsize=(10,8))
sns.barplot(x='mnth', y='cnt', data=bike_df)
plt.title('Bike Rentals by Month')
plt.xlabel('Month')
plt.ylabel('Total Rentals')
plt.show()

#Key Insight:
#Rentals peak during warmer months (e.g., May–August) and Cold months(e.g.,December–February) show a drop in bike usage.

"""###Plotting time series data to analyze trends and seasonality."""

from statsmodels.tsa.seasonal import seasonal_decompose

##converting the dtetime column into datetime format and setting it as index
bike_df_dup=bike_df.copy()
bike_df_dup['dteday'] = pd.to_datetime(bike_df_dup['dteday'],format='mixed')
bike_df_dup.set_index('dteday', inplace=True)
print(bike_df_dup.head())

plt.figure(figsize=(12, 6))
plt.plot(bike_df.index, bike_df['cnt'], label='Daily Rentals', color='blue')
plt.title('Daily Bike Rentals Over Time')
plt.xlabel('Date')
plt.ylabel('Total Rentals')
plt.legend()
plt.grid()
plt.show()

decompose_result = seasonal_decompose(bike_df['cnt'], model='additive', period=365)

decompose_result.seasonal.plot()

decompose_result.trend.plot()

decompose_result.resid.plot()

"""Trend: Rentals show a clear upward trend, reflecting increased popularity.

Seasonality: Rentals are higher in summer and lower in winter, following annual weather patterns.

Residuals: Spikes or dips could indicate outlier events like weather disruptions or holidays.

**Key Insights:**

1)Rentals peak during summer months (May–August) and decline in winter months (December–February).

2)higher rentals on weekends for leisure and higher on weekdays during leisure hours.

3)Rentals are time-sensitive, with peaks during specific hours on working and non-working days.

### Using heatmaps or correlation matrices to understand the correlation between different features.
"""

bike_df

bike_df[numerical_columns].corr()

sns.heatmap(bike_df[numerical_columns].corr(),annot=True,cmap='coolwarm')

"""Insights:

1)Temperature and atmospheric temperature has highest positive correlation(multi collinearity)

2)Registered and cnt temperature has highest positive correlation(multi collinearity)

# **3.Feature Engineering:**

### Generating new features from existing ones to improve model performance.
"""

bike_df

bike_df['dteday'] = pd.to_datetime(bike_df['dteday'],format='mixed')

bike_df['dteday'] = bike_df['dteday'] + pd.to_timedelta(bike_df['hr'], unit='h')

bike_df

"""Dropping the yr,mnth and hr column as the data is included in dteday feature"""

bike_df=bike_df.drop(['yr','mnth','hr'],axis=1)

bike_df

"""### Handling categorical variables through techniques like one-hot encoding or label encoding."""

from sklearn.preprocessing import LabelEncoder

lab=LabelEncoder()

for column in bike_df:
    if bike_df[column].dtype==object:
        bike_df[column]=lab.fit_transform(bike_df[column])

bike_df

"""Since We are using the model that includes decision tree,we are using label encoder

### Scaling or normalizing numerical features to ensure uniformity in their ranges.
"""

from sklearn.preprocessing import MinMaxScaler

minmax=MinMaxScaler()

bike_df.set_index('dteday', inplace=True)

scaled_df=minmax.fit_transform(bike_df)

scaled_features = pd.DataFrame(scaled_df, columns=bike_df.columns)

scaled_features.head()

"""Data is ready to be used for model

# **4.Model Building:**
# **5.Hyperparameter Tuning:**

### Splitting the dataset into training and testing sets for model evaluation.
"""

from sklearn.model_selection import train_test_split

scaled_features

features=scaled_features.drop('cnt',axis=1)
target=scaled_features[['cnt']]

x_train,x_test,y_train,y_test=train_test_split(features,target,train_size=0.8,random_state=100)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""### Selecting appropriate machine learning algorithms such as Decision Tree, Random Forest, and Gradient Boosting Regression for predicting bike rental demand.

#### Decision Tree
"""

from sklearn.tree import DecisionTreeRegressor,plot_tree
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import GridSearchCV

# Training the model
dectree=DecisionTreeRegressor(max_depth=5,min_samples_leaf=3,min_samples_split=5,random_state=100)

dt_model=dectree.fit(x_train,y_train)

## RMSE, MAE, or R-squared
y_pred=dt_model.predict(x_test)
rms_dt=np.sqrt(mean_squared_error(y_test,y_pred))
print("RMSE:",rms_dt)
mae_dt=mean_absolute_error(y_test,y_pred)
print("MAE:",mae_dt)
r2_dt=r2_score(y_test,y_pred)
print("R2:",r2_dt)

## Plottig the decision tree
plt.figure(figsize=(10,8))
plot_tree(dt_model,filled=True,feature_names=x_train.columns)
plt.show()

## finding the best parameter using grid search
params={'max_depth':range(2,10),'min_samples_split':range(2,10),'min_samples_leaf':range(2,10)}
grid=GridSearchCV(dectree,params,verbose=2)

grid.fit(x_train,y_train)

grid.best_params_

## Building decision tree model with best params
dectree2=DecisionTreeRegressor(max_depth=9,min_samples_leaf=3,min_samples_split=2,random_state=100)

dt_model2=dectree2.fit(x_train,y_train)

## RMSE, MAE, or R-squared of best parameters model
y_pred=dt_model.predict(x_test)
rms_dt=np.sqrt(mean_squared_error(y_test,y_pred))
print("RMSE:",rms_dt)
mae_dt=mean_absolute_error(y_test,y_pred)
print("MAE:",mae_dt)
r2_dt=r2_score(y_test,y_pred)
print("R2:",r2_dt)

"""#### Random Forest"""

from sklearn.ensemble import RandomForestRegressor

rand_for=RandomForestRegressor(n_estimators=100,max_depth=5,min_samples_leaf=3,min_samples_split=5,random_state=100,bootstrap=False)

rand_model=rand_for.fit(x_train,y_train)

## RMSE, MAE, or R-squared
y_pred_rand=rand_model.predict(x_test)
rms_rand=np.sqrt(mean_squared_error(y_test,y_pred_rand))
print("RMSE:",rms_rand)
mae_rand=mean_absolute_error(y_test,y_pred_rand)
print("MAE:",mae_rand)
r2_rand=r2_score(y_test,y_pred_rand)
print("R2:",r2_rand)

##using grid search to find best parameters
params_rand={'n_estimators':range(50,100),'bootstrap':[True,False]}
grid_sch_rand=GridSearchCV(rand_for,params_rand,verbose=2)

grid_sch_rand.fit(x_train,y_train)

grid_sch_rand.best_params_

## Building Random Forest model with best params
rand_for2=RandomForestRegressor(n_estimators=98,max_depth=5,min_samples_leaf=3,min_samples_split=5,random_state=100,bootstrap=True)

rand_model2=rand_for2.fit(x_train,y_train)

## RMSE, MAE, or R-squared of best params model
y_pred_rand2=rand_model2.predict(x_test)
rms_rand2=np.sqrt(mean_squared_error(y_test,y_pred_rand2))
print("RMSE:",rms_rand2)
mae_rand2=mean_absolute_error(y_test,y_pred_rand2)
print("MAE:",mae_rand2)
r2_rand2=r2_score(y_test,y_pred_rand2)
print("R2:",r2_rand2)

"""#### Gradient Boosting Regression"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV

gb_model = GradientBoostingRegressor(n_estimators=50,learning_rate=0.01,max_depth=5)

gb_model.fit(x_train,y_train)

## RMSE, MAE, or R-squared
y_pred_gb=gb_model.predict(x_test)
rms_gb=np.sqrt(mean_squared_error(y_test,y_pred_gb))
print("RMSE:",rms_gb)
mae_gb=mean_absolute_error(y_test,y_pred_gb)
print("MAE:",mae_gb)
r2_gb=r2_score(y_test,y_pred_gb)
print("R2:",r2_gb)

gb_param_grid = {'n_estimators': [50,100,150,200,250,300],'learning_rate': [0.01, 0.1, 0.2],'max_depth': [3, 5, 7]}
gb_random_search = RandomizedSearchCV(estimator=gb_model, param_distributions=gb_param_grid,n_iter=10, cv=3, verbose=2)

#Finding the best parameters using Random search
gb_random_search.fit(x_train, y_train)

gb_best_params = gb_random_search.best_params_
gb_best_score = gb_random_search.best_score_
print('gb_best_params=',gb_best_params)
print('gb_best_score=',gb_best_score)

## Building the model with best parameters
gb_model2 = GradientBoostingRegressor(n_estimators=50,learning_rate=0.01,max_depth=5)
gb_model2.fit(x_train,y_train)

## RMSE, MAE, R-squared value of best params model
y_pred_gb2=gb_model2.predict(x_test)
rms_gb2=np.sqrt(mean_squared_error(y_test,y_pred_gb2))
print("RMSE:",rms_gb2)
mae_gb2=mean_absolute_error(y_test,y_pred_gb2)
print("MAE:",mae_gb2)
r2_gb2=r2_score(y_test,y_pred_gb2)
print("R2:",r2_gb2)

"""# **6.Model Evaluation:**

### Compare the performance of different models to select the best one.

Based on the performance of models like Random forest Regressor , Decision Tree and Gradient Boosting Regressor:

-> Random Forest Model yields the best results ,99% of the variability in the target variable is explained by the model

# **7.Model Deployment:**
"""

import pickle

